{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import ks_2samp\n",
    "from sklearn.metrics import f1_score\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r''\n",
    "training_data = 'case_study_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = pd.read_csv(os.path.join(data_path, training_data))\n",
    "td.shape\n",
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_docs(doc):\n",
    "    remove_rule_patterns = r\"([\\s\\W]?[xX]{4}[\\s\\W]?)|([\\s\\.]?[xX]{2}/[xX]{2}/[xX]{2,4}[\\s\\.]?)|([\\s\\W]?[\\(\\)\\{\\}\\,\\.][\\s\\W]?)\"\n",
    "    s1 = re.sub(remove_rule_patterns, ' ', doc.lower())\n",
    "    return re.sub(r'\\s+', ' ', s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  preprocess_tuple_list(tuplist):\n",
    "    \n",
    "    docs, labels = zip(*tuplist)\n",
    "    \n",
    "    vfunc = np.vectorize(preprocess_docs)\n",
    "    pp_doc = list(vfunc(np.array(docs)))\n",
    "    \n",
    "    return list(zip(pp_doc, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_binary_cat(product, target_product):\n",
    "    dict_template = {}\n",
    "    if product==target_product:\n",
    "        dict_template['Y']=True\n",
    "        dict_template['N']=False\n",
    "    else:\n",
    "        dict_template['Y']=False\n",
    "        dict_template['N']=True\n",
    "    return {'cats': dict_template}\n",
    "\n",
    "def assign_category(df, doc_column, label_column):\n",
    "    temp_df = df.copy()\n",
    "    labels = list(set(temp_df[label_column]))\n",
    "    cat_dict = dict(zip(labels, [False] * len(labels)))\n",
    "    temp_df['cats'] = temp_df[label_column].apply(lambda x: set_binary_cat(x, target_product))\n",
    "    tuple_output = list(zip(temp_df[doc_column], temp_df['cats']))\n",
    "    del temp_df\n",
    "    return tuple_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df, target_product, limit, split):\n",
    "    # Data filtered for the target product to build the classifier for.\n",
    "    X_prod_i = df[df['product_group']==target_product].copy()\n",
    "    X_prod_j = df[df['product_group']!=target_product].copy()\n",
    "\n",
    "    #X_prod_i.product_group.value_counts()\n",
    "    #X_prod_j.product_group.value_counts()\n",
    "\n",
    "    # Under sample the Majority class which are complaints submitted for the \n",
    "    # product_label that is not being classified. Obtain a subset of X_prod_j\n",
    "    # the size of X_prod_i stratified by product_label to ensure that the \n",
    "    # distributions of the product_label is preserved in the Undersampled set.\n",
    "    X_prod_j_ss, X_disregard, y_prod_j_ss, y_disregard = train_test_split(\n",
    "                                                            X_prod_j.text, \n",
    "                                                            X_prod_j.product_group, \n",
    "                                                            train_size=len(X_prod_i), \n",
    "                                                            test_size=0.3\n",
    "                                                        )\n",
    "\n",
    "    D_prod_j = X_prod_j.product_group.value_counts(normalize=True)\n",
    "    D_prod_j_train = y_prod_j_ss.value_counts(normalize=True)\n",
    "    test = ks_2samp(D_prod_j, D_prod_j_train, alternative='two-sided', mode='asymp')\n",
    "    if test.pvalue < significance:\n",
    "        print('Product label distribution not preserved in subset!!!')\n",
    "\n",
    "    y_cats_i = [set_binary_cat(product_group, target_product) for product_group in list(X_prod_i.product_group)]\n",
    "    Xy_i = list(zip(X_prod_i.text, y_cats_i))\n",
    "    #Xy_i[0:10]\n",
    "    y_cats_j = [set_binary_cat(product_group, target_product) for product_group in list(y_prod_j_ss)]\n",
    "    Xy_j = list(zip(X_prod_j_ss, y_cats_j))\n",
    "    #Xy_j[0:10]\n",
    "\n",
    "    Xy_agg = Xy_i + Xy_j\n",
    "\n",
    "    Xy_agg_filtered = random.sample(Xy_agg, limit)\n",
    "    train_size = int(len(Xy_agg_filtered)*split)\n",
    "\n",
    "    return Xy_agg_filtered[:train_size], Xy_agg_filtered[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    training_data: list,\n",
    "    test_data: list,\n",
    "    iterations: int,\n",
    "    model_architecture: str\n",
    ") -> None:\n",
    "    # Build pipeline\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    if \"textcat\" not in nlp.pipe_names:\n",
    "        textcat = nlp.create_pipe(\n",
    "            \"textcat\", config={\"exclusive_classes\": True, \"architecture\": model_architecture}\n",
    "        )\n",
    "        nlp.add_pipe(textcat, last=True)\n",
    "    else:\n",
    "        textcat = nlp.get_pipe(\"textcat\")\n",
    "\n",
    "    textcat.add_label('Y')\n",
    "    textcat.add_label('N')\n",
    "    \n",
    "    # Train only textcat\n",
    "    training_excluded_pipes = [\n",
    "        pipe for pipe in nlp.pipe_names if pipe != \"textcat\"\n",
    "    ]\n",
    "    with nlp.disable_pipes(training_excluded_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        # Training loop\n",
    "        print('='*72)\n",
    "        print(\"Beginning training\")\n",
    "        print(\"Loss\\tPrecision\\tRecall\\tF-score\")\n",
    "        batch_sizes = compounding(\n",
    "            4.0, 32.0, 1.001\n",
    "        )  # A generator that yields infinite series of input numbers\n",
    "        for i in range(iterations):\n",
    "            loss = {}\n",
    "            random.shuffle(training_data)\n",
    "            batches = minibatch(training_data, size=batch_sizes)\n",
    "            for batch in batches:\n",
    "                text, labels = zip(*batch)\n",
    "                nlp.update(\n",
    "                    text,\n",
    "                    labels,\n",
    "                    drop=0.2,\n",
    "                    sgd=optimizer,\n",
    "                    losses=loss\n",
    "                )\n",
    "            with textcat.model.use_params(optimizer.averages):\n",
    "                evaluation_results = evaluate_model(\n",
    "                    tokenizer=nlp.tokenizer,\n",
    "                    textcat=textcat,\n",
    "                    test_data=test_data\n",
    "                )\n",
    "                print(\n",
    "                    f\"{loss['textcat']}\\t{evaluation_results['precision']}\"\n",
    "                    f\"\\t{evaluation_results['recall']}\"\n",
    "                    f\"\\t{evaluation_results['f-score']}\"\n",
    "                )\n",
    "    \n",
    "    # Save model\n",
    "    version = 1\n",
    "    model_dir_name = f\".\\\\models\\\\{target_product}_{model_architecture}_model_artifacts_v{version}\"\n",
    "    if os.path.isdir(model_dir_name):\n",
    "        while os.path.isdir(model_dir_name):\n",
    "            version +=1\n",
    "            model_dir_name = f\".\\\\models\\\\{target_product}_{model_architecture}_model_artifacts_v{version}\"\n",
    "    with nlp.use_params(optimizer.averages):\n",
    "        nlp.to_disk(model_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    tokenizer, textcat, test_data: list\n",
    ") -> dict:\n",
    "    reviews, labels = zip(*test_data)\n",
    "    reviews = (tokenizer(review) for review in reviews)\n",
    "    true_positives = 0\n",
    "    false_positives = 1e-8  # Can't be 0 because of presence in denominator\n",
    "    true_negatives = 0\n",
    "    false_negatives = 1e-8\n",
    "    for i, review in enumerate(textcat.pipe(reviews)):\n",
    "        true_label = labels[i]['cats']\n",
    "        for predicted_label, score in review.cats.items():\n",
    "            # Every cats dictionary includes both labels. You can get all\n",
    "            # the info you need with just the pos label.\n",
    "            if (\n",
    "                predicted_label == \"N\"\n",
    "            ):\n",
    "                continue\n",
    "            if score >= 0.5 and true_label[\"Y\"]:\n",
    "                true_positives += 1\n",
    "            elif score >= 0.5 and true_label[\"N\"]:\n",
    "                false_positives += 1\n",
    "            elif score < 0.5 and true_label[\"N\"]:\n",
    "                true_negatives += 1\n",
    "            elif score < 0.5 and true_label[\"Y\"]:\n",
    "                false_negatives += 1\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        f_score = 0\n",
    "    else:\n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f-score\": f_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_product_model_test(input_data, target_product, version=1):\n",
    "    \"\"\"\n",
    "    Takes a string input of a complaint from the CFPB and will\n",
    "    score the complaint using the specified product model version#.\n",
    "    Output will be the a print of the prediction value (\"Y\" or \"N\") \n",
    "    along with the corresponding model score.\n",
    "    \n",
    "    Argumenst:\n",
    "    input: str\n",
    "    target_product: str\n",
    "    version: int\n",
    "    \"\"\"\n",
    "    #  Load saved trained model\n",
    "    model_directory = r'C:\\Users\\vince\\Projects\\wf_case_study\\data\\models'\n",
    "    product_models = os.listdir(model_directory)\n",
    "    loaded_models = [spacy.load(os.path.join(model_directory, model))\n",
    "                     for model in product_models \n",
    "                     if ((model[:len(target_product)] == target_product) & (model[-1:]==str(version)))\n",
    "                    ]\n",
    "    \n",
    "    for model in loaded_models:\n",
    "        # Generate prediction\n",
    "        parsed_text = model(input_data)\n",
    "        # Determine prediction to return\n",
    "        if parsed_text.cats[\"Y\"] > parsed_text.cats[\"N\"]:\n",
    "            prediction = \"Y\"\n",
    "            score = parsed_text.cats[\"Y\"]\n",
    "        else:\n",
    "            prediction = \"N\"\n",
    "            score = parsed_text.cats[\"N\"]\n",
    "        print(f\"Target product: {target_product}\")\n",
    "        print(\n",
    "            f\"Review text: {input_data}\\nComplaint is for {target_product} product?: {prediction}\"\n",
    "            f\"\\tScore: {score}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_model_test(input_data, version=1):\n",
    "    \"\"\"\n",
    "    Takes a string input of a complaint from the CFPB and will\n",
    "    score the complaint using all product models for the specified version#.\n",
    "    Output will be the a print of the final predicted product for the \n",
    "    complaint was filed against. This is defined to be the highest predicted\n",
    "    score among all model scores by product.\n",
    "    \n",
    "    Argumenst:\n",
    "    input: str\n",
    "    version: int\n",
    "    \"\"\"\n",
    "    #  Load saved trained model\n",
    "    model_directory = r'C:\\Users\\vince\\Projects\\wf_case_study\\data\\models'\n",
    "    product_models = [model for model in os.listdir(model_directory) if model[-1:]==str(version)]\n",
    "    loaded_models = [spacy.load(os.path.join(model_directory, model))\n",
    "                     for model in product_models\n",
    "                    ]\n",
    "    print(f'Review text: \\n{input_data}')\n",
    "    \n",
    "    table_headers = ['Product Classifier', 'Prediction', 'Score']\n",
    "    \n",
    "    iter_list = []\n",
    "    i=0\n",
    "    prediction_dict = {}\n",
    "    for model in loaded_models:\n",
    "        intra_iter_list = []\n",
    "        m = re.match(f\"(\\w+)_simple_cnn_model_artifacts_v\\d\", product_models[i])\n",
    "        if m:\n",
    "            current_product = m.group(1)\n",
    "            intra_iter_list.append(current_product)\n",
    "            \n",
    "        prediction_dict[current_product] = {}\n",
    "        \n",
    "        # Generate prediction\n",
    "        parsed_text = model(input_data)\n",
    "        # Determine prediction to return\n",
    "        if parsed_text.cats[\"Y\"] > parsed_text.cats[\"N\"]:\n",
    "            prediction = \"Y\"\n",
    "            score = parsed_text.cats[\"Y\"]\n",
    "        else:\n",
    "            prediction = \"N\"\n",
    "            score = -1*parsed_text.cats[\"N\"]\n",
    "        \n",
    "        prediction_dict[current_product]['prediction'] = prediction\n",
    "        prediction_dict[current_product]['score'] = score\n",
    "        \n",
    "        intra_iter_list.append(prediction)\n",
    "        intra_iter_list.append(score)\n",
    "        \n",
    "        iter_list.append(intra_iter_list)\n",
    "        i+=1\n",
    "    \n",
    "    candidate_predictions = {product: prod_dict['score']\n",
    "                             for product, prod_dict in prediction_dict.items() \n",
    "                             if prod_dict['prediction']=='Y'\n",
    "                            }\n",
    "    highest_score_idx = list(candidate_predictions.values()).index(max(list(candidate_predictions.values())))\n",
    "    predicted_product = list(candidate_predictions.keys())[highest_score_idx]\n",
    "    print('='*51)\n",
    "    print(tabulate(iter_list, headers=table_headers, tablefmt='orgtbl'))\n",
    "    print(f\"\\n!!!!!!!!!!========== AND THE WINNER IS!!!!! ==============!!!!!!!!!!\\n\\n{' '*21}{predicted_product}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_df2(loaded_model, input_text, current_product, preprocess_doc=False):\n",
    "    \n",
    "    if preprocess_doc:\n",
    "        input_doc = str(preprocess_docs(input_text))\n",
    "    else:\n",
    "        input_doc = str(input_text)\n",
    "    # Generate prediction\n",
    "    parsed_text = loaded_model(input_doc)\n",
    "    # Determine prediction to return\n",
    "    \n",
    "    if parsed_text.cats[\"Y\"] > parsed_text.cats[\"N\"]:\n",
    "        prediction = current_product\n",
    "        score = parsed_text.cats[\"Y\"]\n",
    "    else:\n",
    "        prediction = \"other_product\"\n",
    "        score = parsed_text.cats[\"N\"]\n",
    "    \n",
    "    return prediction, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_df(df, text_column, version=1, preprocess_doc=False):\n",
    "    \"\"\"\n",
    "    Takes a dataframe as input as well as the name of the column that contains\n",
    "    consumer complaints from the CFPB and will append columns containing the \n",
    "    predicted value for each product binary classification model along with \n",
    "    the score along with a column for the final predicted product that the\n",
    "    complaint corresponds to.\n",
    "    \n",
    "    Output is the original dataframe along with indicividual binary product\n",
    "    classification prediction and scores and final prediction label.\n",
    "    \n",
    "    Argumenst:\n",
    "    input: str\n",
    "    version: int\n",
    "    \"\"\"\n",
    "    #  Load saved trained models\n",
    "    model_directory = r'C:\\Users\\vince\\Projects\\wf_case_study\\data\\models'\n",
    "    product_models = [model for model in os.listdir(model_directory) if model[-1:]==str(version)]\n",
    "    loaded_models = [spacy.load(os.path.join(model_directory, model))\n",
    "                     for model in product_models\n",
    "                    ]\n",
    "    \n",
    "    # Load copy of data frame and remove any rows where the input text to be\n",
    "    # scored is null.\n",
    "    temp_df = df.copy()\n",
    "    temp_df = temp_df[temp_df[text_column].notnull()]\n",
    "    n = len(temp_df)\n",
    "    \n",
    "    products = []\n",
    "    i=0\n",
    "    vfunc = np.vectorize(prediction_df2)\n",
    "    text_array = np.array(temp_df['text'])\n",
    "    \n",
    "    for model in tqdm(loaded_models):\n",
    "        # Fetch current product for which the model is predicting.\n",
    "        m = re.match(f\"(\\w+)_simple_cnn_model_artifacts_v\\d\", product_models[i])\n",
    "        if m:\n",
    "            current_product = m.group(1)\n",
    "            products.append(current_product)\n",
    "            #print(current_product)\n",
    "          \n",
    "        print(f'version: {version}\\ncurrent_product: {current_product}')\n",
    "        \n",
    "        tuples = vfunc(model, text_array, current_product,preprocess_doc)\n",
    "        temp_df[f'{current_product}_pred'] = list(tuples[0])\n",
    "        temp_df[f'{current_product}_score'] = list(tuples[1])\n",
    "\n",
    "        i+=1\n",
    "    \n",
    "    score_cols = [col for col in temp_df.columns if col[-6:]=='_score']\n",
    "    temp_df['predicted_product'] = temp_df[score_cols].idxmax(axis=1)\n",
    "    temp_df['predicted_product'] = temp_df['predicted_product'].apply(lambda x: x[:-6])\n",
    "    \n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_thresholds(scored_df):\n",
    "    \"\"\"\n",
    "    Used to determine the optimal classification thresholds for a scored dataframe.\n",
    "    \"\"\"\n",
    "    dfc = scored_df.copy()\n",
    "    \n",
    "    products = ['bank_service', 'credit_reporting', 'mortgage', 'money_transfers','credit_card',\n",
    "                     'loan', 'debt_collection']\n",
    "    \n",
    "    threshold_dict = {}\n",
    "    for product in products:\n",
    "        \n",
    "        dfc['true_label'] = np.where(dfc['product_group']==dfc[f'{product}_pred'], 1, 0)\n",
    "        y_true = list(dfc['true_label'])\n",
    "        \n",
    "        dfc['pred_score'] = np.where(dfc[f'{product}_score'] < 0, dfc[f'{product}_score'] + 1, dfc[f'{product}_score'])\n",
    "        y_hat = list(dfc['pred_score'])\n",
    "        \n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_hat)\n",
    "        gmeans = np.sqrt(tpr * (1-fpr))\n",
    "        ix = np.argmax(gmeans)\n",
    "        \n",
    "        threshold_dict[product] = {'threshold': thresholds[ix], 'tpr': tpr, 'fpr': fpr, 'thresholds':thresholds }\n",
    "    \n",
    "    return threshold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_generator(thresh_dict, product):\n",
    "       \n",
    "    # plot the roc curve for the model\n",
    "    pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n",
    "    pyplot.plot(thresh_dict[product]['fpr'], thresh_dict[product]['tpr'], marker='.', label=product)\n",
    "    #pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n",
    "    # axis labels\n",
    "    pyplot.xlabel('False Positive Rate')\n",
    "    pyplot.ylabel('True Positive Rate')\n",
    "    pyplot.legend()\n",
    "    # show the plot\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_metrics(df, threshold_dict):\n",
    "    dfc = df.copy()\n",
    "    products = ['bank_service', 'credit_reporting', 'mortgage', 'money_transfers',\n",
    "                'credit_card', 'loan', 'debt_collection'\n",
    "               ]\n",
    "    \n",
    "    columns = [f'{prod}_score' for prod in products] #+ ['predicted_product']\n",
    "    \n",
    "    total_iter_levels = len(dfc)*len(columns)\n",
    "    table_headers = ['Product Classifer', 'f1', 'Precision', 'Recall', 'TP', 'FP', 'FN', 'TN']\n",
    "    iter_list = []\n",
    "    \n",
    "    pbar = tqdm(total=total_iter_levels)\n",
    "    for col in columns:\n",
    "        product = col[:-6]\n",
    "        true_labels = list(dfc['product_group'])\n",
    "        predicted_score = list(dfc[col])\n",
    "        threshold = threshold_dict[product]['threshold']\n",
    "        \n",
    "        labels=  list(zip(true_labels, predicted_score))\n",
    "        \n",
    "        tp, tn, fp, fn = 0, 0, 1e-8, 1e-8\n",
    "        for label in labels:\n",
    "            if ((label[0]==product) & (label[1]>= threshold)):\n",
    "                tp+=1\n",
    "            elif label[0] == product:\n",
    "                fn+=1\n",
    "            elif label[1] >= threshold:\n",
    "                fp+=1\n",
    "            else:\n",
    "                tn +=1\n",
    "            pbar.update(1)\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        iter_list.append([col[:-6], f1, precision, recall, tp, int(fp), int(fn), tn])\n",
    "    \n",
    "    val_counts = dfc.product_group.value_counts()\n",
    "    print(f'Dataframe size: {len(dfc)}\\nDistribution of product_group:\\n{dfc.product_group.value_counts()}')\n",
    "    #print(tabulate(iter_list, headers=['Product Group', 'Frequency', 'Relative %'], tablefmt='orgtbl'))\n",
    "    print('='*50)\n",
    "    print(tabulate(iter_list, headers=table_headers, tablefmt='orgtbl'))\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(classifers,\n",
    "                          target_names,\n",
    "                          cmap=None,\n",
    "                          normalize=True):\n",
    "    \"\"\"\n",
    "    given a sklearn confusion matrix (cm), make a nice plot\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
    "\n",
    "    target_names: given classification classes such as [0, 1, 2]\n",
    "                  the class names, for example: ['high', 'medium', 'low']\n",
    "\n",
    "    title:        the text to display at the top of the matrix\n",
    "\n",
    "    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
    "                  see http://matplotlib.org/examples/color/colormaps_reference.html\n",
    "                  plt.get_cmap('jet') or plt.cm.Blues\n",
    "\n",
    "    normalize:    If False, plot the raw numbers\n",
    "                  If True, plot the proportions\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n",
    "                                                              # sklearn.metrics.confusion_matrix\n",
    "                          normalize    = True,                # show proportions\n",
    "                          target_names = y_labels_vals,       # list of names of the classes\n",
    "                          title        = best_estimator_name) # title of graph\n",
    "\n",
    "    Citiation\n",
    "    ---------\n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import itertools\n",
    "    plt.figure(figsize=(25, 15))\n",
    "    for i, (key, val) in enumerate(classifers.items()):\n",
    "        cm = val\n",
    "        title = key\n",
    "        plt.subplot(1, len(classifers), i+1)\n",
    "        accuracy = np.trace(cm) / float(np.sum(cm))\n",
    "        misclass = 1 - accuracy\n",
    "        \n",
    "        if cmap is None:\n",
    "            cmap = plt.get_cmap('Blues')\n",
    "        \n",
    "        \n",
    "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "        plt.title(title)\n",
    "        #plt.colorbar()\n",
    "        \n",
    "        if target_names is not None:\n",
    "            tick_marks = np.arange(len(target_names))\n",
    "            plt.xticks(tick_marks, target_names, rotation=45)\n",
    "            plt.yticks(tick_marks, target_names)\n",
    "        \n",
    "        if normalize:\n",
    "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        \n",
    "        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
    "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            if normalize:\n",
    "                plt.text(j, i, \"{:0.3f}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            else:\n",
    "                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label\\naccuracy={:0.3f}; misclass={:0.3f}'.format(accuracy, misclass))\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "project_directory = r'C:\\Users\\vince\\Projects\\wf_case_study\\data'\n",
    "#target_product = 'bank_service'\n",
    "volume_limit = 1500\n",
    "training_iterations = 3\n",
    "model_architecture = 'simple_cnn'\n",
    "significance = 0.05 # Significance for test of Distribution of product_label for undersampled majority class. (load_data)\n",
    "split = 0.7 # Percentage of records to be allocated to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(project_directory)\n",
    "for product in products:\n",
    "    target_product = product\n",
    "    print(f\"working_directory: {project_directory}\\nProduct being trained: {target_product}\\nTrain+Test size limit: {volume_limit}\\nTrain\\\\Test split:{split}\\nModel Training Iterations: {training_iterations}\")\n",
    "    training_data, test_data = load_data(td, target_product, limit=volume_limit, split=split)\n",
    "    train_model(training_data, test_data, iterations=training_iterations, model_architecture=model_architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "project_directory = r'C:\\Users\\vince\\Projects\\wf_case_study\\data'\n",
    "#target_product = 'bank_service'\n",
    "volume_limit = 5000\n",
    "training_iterations = 5\n",
    "model_architecture = 'simple_cnn'\n",
    "significance = 0.05 # Significance for test of Distribution of product_label for undersampled majority class. (load_data)\n",
    "split = 0.7 # Percentage of records to be allocated to training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(project_directory)\n",
    "for product in products:\n",
    "    target_product = product\n",
    "    print(f\"working_directory: {project_directory}\\nProduct being trained: {target_product}\\nTrain+Test size limit: {volume_limit}\\nTrain\\\\Test split:{split}\\nModel Training Iterations: {training_iterations}\")\n",
    "    training_data, test_data = load_data(td, target_product, limit=volume_limit, split=split)\n",
    "    train_model(training_data, test_data, iterations=training_iterations, model_architecture=model_architecture)\n",
    "    #print(\"Testing model\")\n",
    "    #test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STRING_BS = \"\"\"I opened a Bank of the the West account. The account came with a promotion. \n",
    "The promotion was get {$100.00} for setting up a qualifying direct deposit. I have received \n",
    "that promotion. The other promotion was, \" get {$50.00} for making XXXX debit card purchases \n",
    "each month for 2 months. '' I have met the terms for the {$50.00} promotion and have not \n",
    "received the promotion that I am entitled to. I want the {$50.00} that I am entitled to that \n",
    "Bank of the West has not given me.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_STRING_CC = \"\"\"I HAVE HAD AN ACCOUNT WITH DISCOVER CARD SINCE 2011. I HAVE PAID THEM AS AGREED MONTHLY EVER SINCE. HOWEVER, NOW THEY HAVE DECIDED THAT THEY WILL NOT GRANT ME CREDIT ANY LONGER? I HAVE NOW AN AVAILABLE CREDIT LIMIT OF SOME {$1500.00}. THEY REFUSE TO ALLOW ME TO USE THIS ACCOUNT? I AM VERY ANGRY ABOUT THIS TO SAY THE LEAST!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_product_model_test(TEST_STRING_CC, 'bank_service')\n",
    "manual_product_model_test(TEST_STRING_CC, 'credit_reporting')\n",
    "manual_product_model_test(TEST_STRING_CC, 'credit_card')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_model_test(TEST_STRING_BS, 2)\n",
    "#manual_model_test(TEST_STRING_CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample from the original dataframe to validate the models on.\n",
    "eval_df = td.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_model_test(TEST_STRING_CC, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_scored1 = score_df(eval_df, 'text', version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds1 = determine_thresholds(eval_df_scored1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(eval_df_scored1, thresholds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df_scored2 = score_df(eval_df, 'text', version=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds2 = determine_thresholds(eval_df_scored2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metrics(eval_df_scored2, thresholds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_true_1 = list(eval_df_scored1['product_group'])\n",
    "labels_predicted_1 = list(eval_df_scored1['predicted_product'])\n",
    "labels_true_2 = list(eval_df_scored2['product_group'])\n",
    "labels_predicted_2 = list(eval_df_scored2['predicted_product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(labels_true_1, labels_predicted_1, labels=products)\n",
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(labels_true_2, labels_predicted_2, labels=products)\n",
    "cm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix({'Model 1': cm1, 'Model 2': cm2},\n",
    "                          products,\n",
    "                          cmap='YlOrRd',\n",
    "                          normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(list(eval_df_scored1['product_group']), list(eval_df_scored1['predicted_product']), labels=products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(list(eval_df_scored2['product_group']), list(eval_df_scored2['predicted_product']), labels=products)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
